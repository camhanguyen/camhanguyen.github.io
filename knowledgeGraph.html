<!DOCTYPE html>
<html lang="en">

    <head>

        <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
                <meta name="description" content="">
                    <meta name="author" content="">

                        <title>Portfolio: Learner Sourcing</title>

                        <!-- Bootstrap core CSS -->
                        <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

                            <!-- Fonts -->
                            <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
                                <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
                                    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

                                        <!-- Styles for this template -->
                                        <link href="css/resume.min.css" rel="stylesheet">

    </head>

    <body id="page-top">

        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Ha Nguyen</span>
                <span class="d-none d-lg-block">
                    <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.jpg" alt="">
                        </span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#objectives">Objectives</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#role">My Role</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#insight">Insights</a>
                    </li>
                    <li class="nav-item">
                      <a href="index.html"><h3 style="color:white;">&larr;</h3></a>
                </ul>
            </div>
        </nav>

        <div class="container-fluid p-0">
          <!--Objective-->
          <section class="resume-section p-3 p-lg-5 d-flex align-items-center" id="objectives">
              <div class="w-100">
                <p>Advisor: Dr. June Ahn</p>
                <p>Toolbox: R, Figma, Qualtrics, HTML, CSS, JavaScript</p>
                <h2>Objectives</h2>
                <p>The design challenge for this research is to quickly and effectively analyze "practical analytics", which is locally-grounded, education data that can more directly inform instructional improvements for teachers.
                Dual challenges arise in utilizing practical analytics:</p>
                <p>(1) Such data are often expensive and time consuming to process at scale.</p>
                <p>(2) Developing usable analytics that validly relate to actionable, improvement decisions is difficult to achieve in practice.</p>
                <p>Observations of challenges in using practical analytics motivate my projects with Dr. June Ahn in the Design and Partnership Lab: <b>develop systems to improve the processing of school-based education data,
                   using a variety of human-computation and machine learning approaches</b>.</p>
                <p>Within this context, I designed an experiment to answer the questions:</p>
                  <p>(1) To what extent can crowdsourcing effectively process open-ended, middle-grade science assessment data?</p>
                  <p>(2) To what extent can crowdsourcing influence participantsâ€™ understanding of task-related concepts,
                  compared to a control group who did not participate in crowdsourcing?</p>
              </div>
          </section>
          <!--End Objective-->
          <hr class="m-0">

          <!--My Roles-->
          <section class="resume-section p-3 p-lg-5 d-flex align-items-center" id="role">
              <div class="w-100">
                <h2>My Roles</h2>
                <ul>
                <li>Design the crowdsourcing training tasks with 168 participants, who were Education undergraduates with prior experience in assessment grading.</li>
                <li>Apply learning sciences research to design the experimental conditions: (1) Grade &amp; Explain. Participants grade and explain their scores; (2) Grade Only. Participants grade only; and
                  (3) Control. Participants did not grade, but watched an instructional video about a related science concept.</li>
                <li>Develop a coding rubric for the assessment tasks, and design the rubric to be clear enough for graders with no prior experience with the assessment.</li>
                <li>Develop a website for the experiments, where participants completed a training tasks, got randomly assigned to a crowdsourcing condition,
                and completed pre and post-assessments.</p>
                <li>Conduct multivariate analyses and Monte Carlo simulations to understand the impact of crowdsourcing, compared to expert scores.</p>
              </ul>
              </div>
          </section>
          <!--End Roles-->
          <hr class="m-0">

          <!--Insights-->
          <section class="resume-section p-3 p-lg-5 d-flex align-items-center" id="insight">
              <div class="w-100">
                <h2>Insights</h2>

                <h3>Crowdsourced assessments can work!</h3>
                <p>Participants provided results with high accuracy and close agreement with expert scores.</p>
                <p>In subsequent simulations, we found that aggregating scores from multiple crowdworkers consistently got us closer to the expert scores,
                  suggesting potential benefit of utilizing collective intelligence in educational domains.</p>
                <img class="img-fluid" src="img/learnerSource.jpg">
                <br></br>

                <h3>Crowdsourced participants learned from the tasks.</h3>
                <p>Compared to those who did not participate in the crowdsourcing tasks, we found positive learning outcomes for both of the Grading groups,
                  and a significant outcome for the Grade Only group,
                  after accounting for pre-test science understanding and attitudes towards science.</p>

                <h3>Opportunities for HCI and learning analytics research.</h3>
                <p>We did not find significant differences between the two grading conditions (Grade Only and Grade &amp; Explain), contrary to prior work in learning sciences
                  and crowdsourcing in general domains.</p>
                <p>This suggests a need to attend to different task design features when it comes to education assessments,
                  such as providing more timely feedback, segmenting task difficulties, or giving opportunities for peer assessments.</p>
                <p>Because the crowdsourced scores may show wide deviation from one another,
                  we are also looking into ways to represent variance in data output to end-users (i.e., teachers), to make more informed instructional decisions at scale.</p>

                <p><i>The preliminary findings from our work can be found in a <a href="https://doi.org/10.1145/3386527.3406734" target="_blank">paper</a> we presented at the Learning@Scale Conference' 20.</i></p>
              </div>
          </section>
          <!--End Insights-->

          <!-- Bootstrap core JavaScript -->
          <script src="vendor/jquery/jquery.min.js"></script>
          <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

          <!-- Plugin JavaScript -->
          <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

          <!-- Main scripts -->
          <script src="js/resume.min.js"></script>
          <script src="js/filter.js"></script>


          </body>

        </html>
